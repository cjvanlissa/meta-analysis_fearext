---
title: "Untitled"
output: bookdown::html_document2
date: '`r format(Sys.time(), "%d %B, %Y")`'
bibliography: references.bib
knit: worcs::cite_all
---

```{r setup, include=FALSE}
run_everything = FALSE
library("worcs")
library(ggplot2)
library(kableExtra)
library(DT)
library(tidySEM)
library(knitr)
knitr::opts_chunk$set(echo = FALSE)
```


```{r prepdata, eval=run_everything}
library(metafor)
library(pema)

# We recommend that you prepare your raw data for analysis in 'prepare_data.R',
# and end that file with either open_data(yourdata), or closed_data(yourdata).
# Then, uncomment the line below to load the original or synthetic data
# (whichever is available), to allow anyone to reproduce your code:
load_data()
dat$Process <- ordered(dat$Process, levels = unique(dat$Process))
processes <- levels(dat$Process)
totaln <- dat[!duplicated(dat$article.code), c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group", "Process")]
totaln <- sapply(processes, function(p){ sum(totaln[totaln$Process == p, -3], na.rm = TRUE)})

dat <- escalc(measure = "SMD",
         n1i = dat$number.of.animals.in.control.group,
         n2i = dat$number.of.animals.in.intervention.group,
         m1i = dat$mean.control.group,
         m2i = dat$mean.experimental.group,
         sd1i = dat$sd.control.group,
         sd2i = dat$sd.experimental.group,
         data = dat)
# dat[c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group", "mean.control.group", "mean.experimental.group", "sd.control.group", "sd.experimental.group")] <- NULL

# Rename
rnm <- read.csv("../rename.csv", stringsAsFactors = F, encoding = "UTF-8")
names(rnm) <- c("orig", "Rename")
mods <- c("type.of.ssri", "frequency", "disease.induction", "species", 
"type.of.test")
for(v in mods){
  these <- rnm[grepl(v, rnm$orig),]
  orig_levs <- gsub(v, "", these$orig)
  nulev <- gsub("^.+?\\.", "", these$Rename)
  levels(dat[[v]]) <- nulev[match(orig_levs, levels(dat[[v]]))]
}

names(dat)[match(mods, names(dat))] <- c("ssri", "freq", "disease", "species", "test")
dat <- dat[!is.na(dat$yi), ]
zscores <- scale(dat[["yi"]])
maxz <- max(zscores)
dat <- dat[-which.max(zscores), ]
dat$id_study <- as.integer(factor(trimws(gsub("\\(\\d+\\)$", "", as.character(dat$article.code)))))
dat$id_es <- 1:nrow(dat)
saveRDS(dat, "dat.RData")
```

```{r}
dat <- readRDS("dat.RData")
processes <- levels(dat$Process)
mods <- c("ssri", "freq", "disease", "species", "test")
```


<!--   Information regarding this data file for the SR fear learning & SSRIs -->

<!-- This data file is organised by fear learning process. This means that every fear learning process has its own Excel sheet. -->

<!-- Within these sheets you find the same layout. The different columns are explained below: -->

<!--   Study name -->
<!-- Article code -->
<!-- Type of SSRI -->
<!-- Frequency -->
<!-- Disease induction -->
<!-- Species -->
<!-- Type of test -->
<!-- Sensitivity analysis -->



<!-- Analyses we want to perform: -->
<!-- 2. Subgroup analyses of the five factors (SSRI, frequency, disease induction, species, type of test) per fear learning process -->
<!-- 3. Check for multi collinearity -->

```{r threelev, echo=FALSE, warning=FALSE, message=FALSE, results="hide", eval = run_everything}
mlm <- lapply(processes, function(p){
  df <- dat[dat$Process == p, ]
  #Conduct meta-analyses
  #model.mods <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), data=df) 
  model.full <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), data=df) 
  model.within_null <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), sigma2=c(NA,0), data = df) 
  model.between_null <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), sigma2=c(0,NA), data = df) 
model.both_null <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), sigma2=c(0,0), data = df) 
#model.mods <- rma.mv(yi, vi, mods = as.formula(paste0("~ ", paste(moderators, collapse = " + "))), random = list(~ 1 | id_study, ~ 1 | id_es), data = df) 
#ggplot(, aes(x=d, colour=interventioncode))+geom_density()
#anova(model.full,rma.mv(yi, vi, mods = ~interventioncode, random = list(~ 1 | id_study, ~ 1 | id_es), data = df) ) 
aov_within <- anova(model.full,model.within_null) 
aov_between <- anova(model.full,model.between_null) 
aov_bothnull <- anova(model.full,model.both_null) 
aov_table <- data.frame(rbind(
c(aov_between$fit.stats.f[c(3:4, 1)], LRT = NA, p = NA),
c(aov_within$fit.stats.r[c(3:4, 1)], LRT = aov_within$LRT, p = aov_within$pval),
c(aov_between$fit.stats.r[c(3:4, 1)], LRT = aov_between$LRT, p = aov_between$pval),
c(aov_bothnull$fit.stats.r[c(3:4, 1)], LRT = aov_bothnull$LRT, p = aov_bothnull$pval)
))
rownames(aov_table) <- c("Three-level model", "Within-studies variance constrained", "Between-studies variance constrained", "Both variance components constrained")
write.csv(aov_table, paste0("threelevel_ma_", p, ".csv"))
confints <- confint(model.full)
#CHeck convergence of variance components:
#par(mfrow=c(2,1))
#plot.profile1 <- profile(model.full, sigma2=1)
#plot.profile2 <- profile(model.full, sigma2=2)

#Write forest plot to file
xname <- paste0("Hedges' g (", p, ")")
tmp <- df[order(df$vi, decreasing = TRUE), ]
df_es <- data.frame(
    Study = ordered(tmp$article.code, levels = unique(tmp$article.code)),
    y = 1:nrow(tmp),
    es = tmp$yi,
    lb = tmp$yi - 1.96*sqrt(tmp$vi),
    ub = tmp$yi + 1.96*sqrt(tmp$vi),
    Process = p)

dfoverall <- data.frame(x = c(model.full$ci.ub, model.full$b[1,1], model.full$ci.lb, model.full$b[1,1]),
                        y = c(max(df_es$y)+2, max(df_es$y)+2.5, max(df_es$y)+2, max(df_es$y)+1.5))
ylabsize = 10
if(p == "Acq retr to ctx") ylabsize =1
pforest <- ggplot(data=df_es)+ 
  geom_point(aes(y=y, x=es))+ 
  geom_errorbarh(aes(y = y, xmin=lb, xmax=ub), height=.1)+
  
  # geom_point(data=data.frame(y = max(df_es$y)+2, es = model.full$b[1,1]), aes(y=y, x=es), size=4, shape = 15, fill = "black")+ 
  # geom_errorbarh(data = data.frame(y = max(df_es$y)+2, lb = model.full$ci.lb, ub = model.full$ci.ub), aes(y = y, xmin=lb, xmax=ub), height=.4, size = 2)+
  geom_polygon(data = dfoverall, aes(x = x, y = y))+
  scale_x_continuous(name=xname, limits = c(min(df_es$lb), min(c(10,max(df_es$ub)))))+
  scale_y_continuous(name = "", breaks=1:max(df_es$y), labels = df_es$Study, trans="reverse")+
  #adding a vertical line at the effect = 0 mark
  geom_vline(xintercept=0, color="black", linetype="dashed", alpha=.5)+
  geom_hline(yintercept=max(df_es$y)+1)+
  theme(axis.text.y = element_text(size=ylabsize))+
  theme_minimal()

ggsave(paste0("threelevel_ma_forest", p, ".png"), pforest, device = "png")
if(p == "Acq retr to ctx") ggsave(paste0("threelevel_ma_forest", p, ".png"), pforest, device = "png", width = 7, height = 12)
res = data.frame(Variance = c("Overall ES", "V within", "V between"),
                 rbind(c(estimate = model.full$b[1,1], ci.lb = model.full$ci.lb, ci.ub = model.full$ci.ub, model.full$pval),
                       c(confints[[1]]$random[1,], aov_table$p[3]),
                       c(confints[[2]]$random[1,], aov_table$p[2])))
  list(mod = model.full,
    aov_table = aov_table,
       res = res)     
})
saveRDS(mlm, "mlm.RData")

tb <- lapply(1:length(processes), function(i){
  tb <- mlm[[i]]$res
  tb$Process <- processes[i]
  tb
})
tb <- do.call(rbind, tb)
tb$CI <- tidySEM::conf_int(lb = tb$ci.lb, ub = tb$ci.ub)
tb[c("ci.lb", "ci.ub")] <- NULL
names(tb) <- c("Parameter", "Estimate", "p", "Process", "CI")
write.csv(tb, "tab_threelevel.csv", row.names = FALSE)
```
```{r, include = FALSE}
num_effect_sizes <- table(table(dat$article.code))
```



## Descriptive statistics

The effect size estimates ranged from `r formatC(min(dat[["yi"]]), digits = 2, format = "f")` to `r formatC(max(dat[["yi"]]), digits = 2, format = "f")` ($M `r report(mean(dat[["yi"]]))`, SD `r report(sd(dat[["yi"]]))`$). 
One study had an effect size estimate almost 15 standard deviations from the mean effect size ($Z = 14.98$); it was removed as an outlier.
Sample sizes ranged from `r min(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")]))` to `r max(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")]))` animals ($M = `r formatC(mean(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")])), digits = 2, format = "f")`, SD = `r formatC(sd(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")])), digits = 2, format = "f")`$). Several studies reported multiple effect sizes (`r min(names(num_effect_sizes)) ` - `r max(names(num_effect_sizes))`, with most reporting effect size).

## Meta-analysis 

Meta-analysis was conducted in R [@rcore] using the R-packages `metafor` [@viechtbauerConductingMetaanalysesMetafor2010], and `pema` [@refpema].
To estimate overall effects, we used three-level meta-analysis to account for dependent effect sizes within studies [@vandennoortgateMetaanalysisMultipleOutcomes2015].
Let $y_{jk}$ denote the $j$ observed effect sizes $y$, originating from $k$ studies.
The multi-level model is then given by the following equations: 
<!--
\begin{center}
$\begin{equation}
\left.
\begin{aligned}
y_i &= \theta_i + \epsilon_i &\text{where } \epsilon_i &\sim N(0, \sigma^2_i)\\
\theta_i &= \mu + \zeta_i &\text{where } \zeta_i &\sim N(0, \tau^2)
\end{aligned}
\right\}
\end{equation}$
\end{center}
-->

$$
      \left.
      \begin{aligned}
        y_{jk} &= \beta_{jk} + \epsilon_{jk} &\text{where } \epsilon_{jk} &\sim N(0, \sigma^2_{\epsilon_{jk}})\\
        \beta_{jk} &= \theta_k + w_{jk} &\text{where } w_{jk} &\sim N(0, \sigma^2_{w})\\
        \theta_{k} &= \delta + b_{k} &\text{where } b_k &\sim N(0, \sigma^2_{b})
      \end{aligned}
      \right\}
$$

The first equation indicates that observed effect sizes are equal to the underlying population effect size, plus sampling error $\epsilon_{jk}$. The second equation indicates that population effect sizes within studies are a function of a study-specific true effect size, plus within-study residuals $w_{jk}$. The third equation indicates that the distribution of study-specific true effect sizes are distributed around an overall mean effect, with between-study residuals $b_k$.

Separate meta-analyses were conducted for each of the fear learning processes.
The overall pooled effect sizes were:

```{r, results = "asis"}
mlm <- readRDS("mlm.RData")
tb <- read.csv("tab_threelevel.csv", stringsAsFactors = FALSE)
# kbl(tb[, c("Parameter", "Estimate", "CI", "p", "Process")], digits = 2) |>
#   kable_styling(bootstrap_options = c("striped", "hover"))
datatable(tb, rownames= FALSE, options = list(
              "pageLength" = nrow(tb))) |>
  formatRound(columns=c('Estimate', 'p'), digits=2)
```

The overall effect size estimate differed significantly from zero only for `r paste0(tb[tb$Parameter == "Overall ES", ]$Process[tb[tb$Parameter == "Overall ES", ]$p < .05], collapse = ", ")`.

The within-studies variance component $\sigma^2_w$ (between effect sizes) was significant only for `r paste0(tb[tb$Parameter == "V within", ]$Process[tb[tb$Parameter == "V within", ]$p < .05], collapse = ", ")`.

The between-studies variance $\sigma^2_b$ was significant for all processes, except `r paste0(tb[tb$Parameter == "V between", ]$Process[tb[tb$Parameter == "V between", ]$p > .05], collapse = ", ")`.

This indicates that there was substantial heterogeneity between average effect sizes across studies, but not between effect sizes published within the same studies.

## Forest plots

The forest plots for the aforementioned three-level meta-analyses are presented below.
Within each plot, studies are ranked by their sampling variance $vi$;
thus, the most precise estimates are at the bottom, near the overall effect.


```{r runplots, include=FALSE}
out = NULL
for (i in 1:length(processes)) {
  out = c(out, knit_expand('forest.rmd'))
}
```

`r paste(knit(text = out), collapse = '\n')`

## Moderator analyses

Five categorical moderators were coded: (SSRI, frequency, disease induction, species, type of test).
The effect of these moderators was investigated using meta-regression.
Note however that 77 dummy variables were required to encode all conditions of these categorical moderators.
As this exceeded in many cases the number of available effect sizes (per process),
these models were not identified.
Addressing this problem requires performing variable selection.
Three steps were taken to do so. 
First, categories that did not occur within one subset of the data (by process) were omitted, as their dummy variables are constant for that specific dataset.
Secondly, some dummy variables were redundant because some studies had identical values on multiple dummy variables.
Only one of these redundant dummy variables was retained, and its name was updated to reflect all redundant dummies it represents.
Thirdly, despite these measures, many meta-regression models dropped all or some of the predictors, suggesting the models were empirically non-identified.
Although these models are available in the online supplemental materials,
we advise against their substantive interpretation.

The problems with meta-regression suggests that a technique is required that performs variable selection during analysis.
Such a technique was recently developed: Bayesian penalized meta-regression (BRMA), as implemented in the `pema` R-package (@refpema).
By imposing a regularizing (horseshoe) prior on the regression coefficients,
BRMA shrinks all coefficients towards zero, which aids empirical model identification.
Coefficients must overwhelm the prior in order to become significantly different from zero.
Thus, this method also performs variable selection: identifying which moderators are important in predicting the effect size.
The resulting regression coefficients are negatively biased by design, but the estimate of residual heterogeneity $\tau^2$ is unbiased.
Note that, as this is a Bayesian model, inference is based on credible intervals.
A credible interval is interpreted as follows: The population value falls within this interval with 95% probability (certainty).
This is different from the interpretation of frequentist confidence intervals, which are interpreted as follows: In the long run, 95% of confidence intervals contain the population value.

Because the within-studies variance component was non-significant in most three-level analyses,
and because most studies only had one effect size, we dropped the third level in the moderator analyses.

```{r, eval = run_everything}
library(metacart)
modres <- lapply(processes, function(p){
  df <- dat[dat$Process == p, ]
  modmat <- model.matrix(~., df[mods])[,-1]
  constcols <- colSums(modmat) == 0 | colSums(modmat) == nrow(modmat)
  constcolnames <- colnames(modmat)[constcols]
  modmat <- modmat[, !constcols]
  modmatlist <- as.list(data.frame(modmat))
  dupcols <- duplicated(modmatlist)
  modmat <- modmat[, !dupcols]
  these_dups <- modmatlist[dupcols]
  modmatlist <- as.list(data.frame(modmat))
  for(i in 1:length(these_dups)){
    idnum <- which(duplicated(c(modmatlist, these_dups[i]), fromLast = T))
    colnames(modmat)[idnum] <- paste0(colnames(modmat)[idnum], ";", names(these_dups)[i])
  }
  res <- rma(yi = df$yi,
             vi = df$vi,
             mods = modmat)
  vifs <- vif(res)
  tabres <- data.frame(Parameter = c(rownames(res$b), "Tau2"),
                       cbind(
                         Estimate = c(res$b, res$tau2),
                         se = c(res$se, res$se.tau2),
                         ci.lb = c(res$ci.lb, res$tau2-1.96*res$se.tau2),
                         ci.ub = c(res$ci.ub, res$tau2+1.96*res$se.tau2),
                         p = c(res$pval, res$QEp)))
  tabres$CI <- conf_int(lb = tabres$ci.lb, ub = tabres$ci.ub)
  tabres[c("ci.lb", "ci.ub")] <- NULL
  tabres$VIF <- c(NA, vifs$vif, NA)
  write.csv(tabres, paste0("rma_mods_", p, ".csv"))
  
  
  moddat <- data.frame(df[c("yi", "vi")], modmat)
  pma <- brma(yi ~., data = moddat)
  sumpma <- data.frame(summary(pma)$coefficients)
  sumpma <- cbind(rownames(sumpma), sumpma)
  names(sumpma)[1:3] <- c("Parameter", "Estimate", "se")
  sumpma <- sumpma[!sumpma$Parameter == "tau", ]
  rownames(sumpma) <- NULL
  sumpma$CI <- conf_int(lb = sumpma$X2.5., ub = sumpma$X97.5.)
  pstars <- c("*", "")[as.integer(apply(sumpma[, c("X2.5.", 
                                                   "X97.5.")], 1, function(x) {
                                                     sum(sign(x)) == 0
                                                   })) + 1]
  sumpma$p <- pstars
  sumpma[c("sd", grep("^X", names(sumpma), value = T))] <- NULL
  write.csv(sumpma, paste0("brma_mods_", p, ".csv"))
  moddatvi <- moddat$vi
  moddat <- moddat[, -which(colnames(moddat) == "vi")]
  resmc <- suppressWarnings(metacart::FEmrt(yi~., data = moddat, vi = moddatvi))
  list(rma = tabres,
       brma = sumpma,
       mc = resmc)
})
saveRDS(modres, "modres.RData")

```

### Classic meta-regression

```{r}
modres <- readRDS("modres.RData")
```

Note that analyses containing VIF values greater than 5 should be regarded as problematic, due to multicolinearity.
This applies to nearly all models.

```{r, results = "asis", eval = FALSE}

htmltools::tagList(
  lapply(1:length(processes), function(i) {
    p <- processes[i]
    tb <- modres[[i]]$rma
    tb$format <- as.integer(tb$p < .05)
    formatStyle(
      formatRound(
        datatable(
          tb,
          rownames = FALSE,
          caption = paste0("Meta-regression coefficients for ", p),
          options = list(columnDefs = list(list(
            targets = 6, visible = FALSE
          )),
          "pageLength" = nrow(tb))
        ),
        columns = c('Estimate', "se", 'p', "VIF"),
        digits = 2
      ),
      "format",
      target = "row",
      backgroundColor = styleEqual(c(0, 1), c('gray', 'white'))
    )
  })
)

```


```{r run-numeric-md, include=FALSE}
out = NULL
for (i in 1:length(processes)) {
  out = c(out, knit_expand('tab_rma_template.rmd'))
}
```

`r paste(knit(text = out), collapse = '\n')`

### Bayesian regularized meta-regression:

```{r, results = "asis", eval = FALSE}
for(i in 1:length(processes)){
  p <- processes[i]
  tb <- modres[[i]]$brma
  tb$format <- as.integer(tb$p == "*")
  print({datatable(tb, rownames= FALSE, caption = paste0("Bayesian regularized meta-regression coefficients for ", p), options = list(
              "pageLength" = nrow(tb))) |>
  formatRound(columns=c('Estimate', 'p'), digits=2)})
}
htmltools::tagList(
  lapply(1:length(processes), function(i) {
    p <- processes[i]
    tb <- modres[[i]]$brma
    tb$format <- as.integer(tb$p < .05)
    formatStyle(
      formatRound(
        datatable(
          tb,
          rownames = FALSE,
          caption = paste0("Bayesian regularized meta-regression coefficients for ", p),
          options = list(columnDefs = list(list(
            targets = 7, visible = FALSE
          )),
          "pageLength" = nrow(tb))
        ),
        columns = c('Estimate', "se"),
        digits = 2
      ),
      "format",
      target = "row",
      backgroundColor = styleEqual(c(0, 1), c('gray', 'white'))
    )
  })
)

```


```{r run-numeric-md2, include=FALSE}
out = NULL
for (i in 1:length(processes)) {
  out = c(out, knit_expand('tab_brma_template.rmd'))
}
```

`r paste(knit(text = out), collapse = '\n')`

# Conclusion

Based on three-level multilevel meta-analyses,
there is limited evidence that overall effects are non-zero in the population,
except for the processes "Acq retr to ctx" and "Ext retr to ctx".
All processes showed significant between-studies variance, except "Ext retr to cue".
Conversely, none of the processes showed significant within-studies variance, except "Acq retr to ctx".
There is thus substantial evidence that heterogeneity in effect sizes is mostly due to between-studies differences.

Classic meta-regression analyses were largely invalid for moderator analysis,
because of high multicolinearity among predictors.
BRMA analyses were used, which are robust to multicolinearity, and perform variable selection by shrinking regression coefficients towards zero.
These BRMA analyses revealed no consistent evidence of any significant moderator effect across processes.
To exclude the possibility that multiple dummies interacted together,
I conducted additional sensitivity analyses using `metaCART` - a decision-tree based algorithm that inherently accommodates interactions between dummies.
However, the `metaCART` analyses also did not find evidence of interaction effects.
