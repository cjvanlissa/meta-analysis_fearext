---
title: "Untitled"
output: bookdown::html_document2
date: '`r format(Sys.time(), "%d %B, %Y")`'
bibliography: references.bib
knit: worcs::cite_all
---

```{r setup, include=FALSE}
run_everything = FALSE
library("worcs")
library(metafor)
library(ggplot2)
library(kableExtra)
library(tidySEM)
# We recommend that you prepare your raw data for analysis in 'prepare_data.R',
# and end that file with either open_data(yourdata), or closed_data(yourdata).
# Then, uncomment the line below to load the original or synthetic data
# (whichever is available), to allow anyone to reproduce your code:
load_data()
dat$Process <- ordered(dat$Process, levels = unique(dat$Process))
processes <- levels(dat$Process)
totaln <- dat[!duplicated(dat$article.code), c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group", "Process")]
totaln <- sapply(processes, function(p){ sum(totaln[totaln$Process == p, -3], na.rm = TRUE)})

dat <- escalc(measure = "SMD",
         n1i = dat$number.of.animals.in.control.group,
         n2i = dat$number.of.animals.in.intervention.group,
         m1i = dat$mean.control.group,
         m2i = dat$mean.experimental.group,
         sd1i = dat$sd.control.group,
         sd2i = dat$sd.experimental.group,
         data = dat)
# dat[c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group", "mean.control.group", "mean.experimental.group", "sd.control.group", "sd.experimental.group")] <- NULL
dat <- dat[!is.na(dat$yi), ]
zscores <- scale(dat[["yi"]])
maxz <- max(zscores)
dat <- dat[-which.max(zscores), ]
dat$id_study <- as.integer(factor(trimws(gsub("\\(\\d+\\)$", "", as.character(dat$article.code)))))
dat$id_es <- 1:nrow(dat)
knitr::opts_chunk$set(echo = TRUE)
```


<!--   Information regarding this data file for the SR fear learning & SSRIs -->

<!-- This data file is organised by fear learning process. This means that every fear learning process has its own Excel sheet. -->

<!-- Within these sheets you find the same layout. The different columns are explained below: -->

<!--   Study name -->
<!-- Article code -->
<!-- Type of SSRI -->
<!-- Frequency -->
<!-- Disease induction -->
<!-- Species -->
<!-- Type of test -->
<!-- Sensitivity analysis -->



<!-- Analyses we want to perform: -->
<!-- 2. Subgroup analyses of the five factors (SSRI, frequency, disease induction, species, type of test) per fear learning process -->
<!-- 3. Check for multi collinearity -->

```{r threelev, echo=FALSE, warning=FALSE, message=FALSE, results="hide", eval = run_everything}
mlm <- lapply(processes, function(p){
  df <- dat[dat$Process == p, ]
  #Conduct meta-analyses
  #model.mods <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), data=df) 
  model.full <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), data=df) 
  model.within_null <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), sigma2=c(NA,0), data = df) 
  model.between_null <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), sigma2=c(0,NA), data = df) 
model.both_null <- rma.mv(yi, vi, random = list(~ 1 | id_study, ~ 1 | id_es), sigma2=c(0,0), data = df) 
#model.mods <- rma.mv(yi, vi, mods = as.formula(paste0("~ ", paste(moderators, collapse = " + "))), random = list(~ 1 | id_study, ~ 1 | id_es), data = df) 
#ggplot(, aes(x=d, colour=interventioncode))+geom_density()
#anova(model.full,rma.mv(yi, vi, mods = ~interventioncode, random = list(~ 1 | id_study, ~ 1 | id_es), data = df) ) 
aov_within <- anova(model.full,model.within_null) 
aov_between <- anova(model.full,model.between_null) 
aov_bothnull <- anova(model.full,model.both_null) 
aov_table <- data.frame(rbind(
c(aov_between$fit.stats.f[c(3:4, 1)], LRT = NA, p = NA),
c(aov_within$fit.stats.r[c(3:4, 1)], LRT = aov_within$LRT, p = aov_within$pval),
c(aov_between$fit.stats.r[c(3:4, 1)], LRT = aov_between$LRT, p = aov_between$pval),
c(aov_bothnull$fit.stats.r[c(3:4, 1)], LRT = aov_bothnull$LRT, p = aov_bothnull$pval)
))
rownames(aov_table) <- c("Three-level model", "Within-studies variance constrained", "Between-studies variance constrained", "Both variance components constrained")
write.csv(aov_table, paste0("threelevel_ma_", p, ".csv"))
confints <- confint(model.full)
#CHeck convergence of variance components:
#par(mfrow=c(2,1))
#plot.profile1 <- profile(model.full, sigma2=1)
#plot.profile2 <- profile(model.full, sigma2=2)

#Write forest plot to file
xname <- paste0("Hedges' g (", p, ")")
tmp <- df[order(df$vi, decreasing = TRUE), ]
df_es <- data.frame(
    Study = ordered(tmp$article.code, levels = unique(tmp$article.code)),
    y = 1:nrow(tmp),
    es = tmp$yi,
    lb = tmp$yi - 1.96*sqrt(tmp$vi),
    ub = tmp$yi + 1.96*sqrt(tmp$vi),
    Process = p)

dfoverall <- data.frame(x = c(model.full$ci.ub, model.full$b[1,1], model.full$ci.lb, model.full$b[1,1]),
                        y = c(max(df_es$y)+2, max(df_es$y)+2.5, max(df_es$y)+2, max(df_es$y)+1.5))

pforest <- ggplot(data=df_es)+ 
  geom_point(aes(y=y, x=es))+ 
  geom_errorbarh(aes(y = y, xmin=lb, xmax=ub), height=.1)+
  
  # geom_point(data=data.frame(y = max(df_es$y)+2, es = model.full$b[1,1]), aes(y=y, x=es), size=4, shape = 15, fill = "black")+ 
  # geom_errorbarh(data = data.frame(y = max(df_es$y)+2, lb = model.full$ci.lb, ub = model.full$ci.ub), aes(y = y, xmin=lb, xmax=ub), height=.4, size = 2)+
  geom_polygon(data = dfoverall, aes(x = x, y = y))+
  scale_x_continuous(name=xname, limits = c(min(df_es$lb), min(c(10,max(df_es$ub)))))+
  scale_y_continuous(name = "", breaks=1:max(df_es$y), labels = df_es$Study, trans="reverse")+
  #adding a vertical line at the effect = 0 mark
  geom_vline(xintercept=0, color="black", linetype="dashed", alpha=.5)+
  geom_hline(yintercept=max(df_es$y)+1)+
  theme_minimal()

ggsave(paste0("threelevel_ma_forest", p, ".png"), pforest, device = "png")
res = data.frame(Variance = c("Overall ES", "V within", "V between"),
                 rbind(c(estimate = model.full$b[1,1], ci.lb = model.full$ci.lb, ci.ub = model.full$ci.ub, model.full$pval),
                       c(confints[[1]]$random[1,], aov_table$p[3]),
                       c(confints[[2]]$random[1,], aov_table$p[2])))
  list(mod = model.full,
    aov_table = aov_table,
       res = res)     
})
saveRDS(mlm, "mlm.RData")
```
```{r, include = FALSE}
num_effect_sizes <- table(table(dat$article.code))
```



## Descriptive statistics

The effect size estimates ranged from `r formatC(min(dat[["yi"]]), digits = 2, format = "f")` to `r formatC(max(dat[["yi"]]), digits = 2, format = "f")` ($M = `r report(mean(dat[["yi"]]))`, SD = `r report(sd(dat[["yi"]]))`$). 
One study had an effect size estimate more than standard deviations from the mean effect size ($Z = `r max(zscores)`$); it was removed as an outlier.
Sample sizes ranged from `r min(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")]))` to `r max(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")]))` animals ($M = `r formatC(mean(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")])), digits = 2, format = "f")`, SD = `r formatC(sd(rowSums(dat[, c("number.of.animals.in.control.group", "number.of.animals.in.intervention.group")])), digits = 2, format = "f")`$). Several studies reported multiple effect sizes (`r min(names(num_effect_sizes)) ` - `r max(names(num_effect_sizes))`, with most reporting effect size).

## Meta-analysis 

Meta-analysis was conducted in R [@r_core_team_r:_2017-1] using the R-packages `metafor` [@viechtbauer_conducting_2010-1], and `pema` [@van_lissa_metaforest:_2017-1], following the recommendations of Field and Gillett (2010).

## Three-level meta-analysis

First, we used three-level meta-analysis to account for dependent effect sizes within studies [@van_den_noortgate_meta-analysis_2015].
Let $y_{jk}$ denote the $j$ observed effect sizes $y$, originating from $k$ studies.
The multi-level model is then given by the following equations: 
<!--
\begin{center}
$\begin{equation}
\left.
\begin{aligned}
y_i &= \theta_i + \epsilon_i &\text{where } \epsilon_i &\sim N(0, \sigma^2_i)\\
\theta_i &= \mu + \zeta_i &\text{where } \zeta_i &\sim N(0, \tau^2)
\end{aligned}
\right\}
\end{equation}$
\end{center}
-->

$$
      \left.
      \begin{aligned}
        y_{jk} &= \beta_{jk} + \epsilon_{jk} &\text{where } \epsilon_{jk} &\sim N(0, \sigma^2_{\epsilon_{jk}})\\
        \beta_{jk} &= \theta_k + w_{jk} &\text{where } w_{jk} &\sim N(0, \sigma^2_{w})\\
        \theta_{k} &= \delta + b_{k} &\text{where } b_k &\sim N(0, \sigma^2_{b})
      \end{aligned}
      \right\}
$$

The first equation indicates that observed effect sizes are equal to the underlying population effect size, plus sampling error $\epsilon_{jk}$. The second equation indicates that population effect sizes within studies are a function of a study-specific true effect size, plus within-study residuals $w_{jk}$. The third equation indicates that the distribution of study-specific true effect sizes are distributed around an overall mean effect, with between-study residuals $b_k$.

Separate meta-analyses were conducted for each of the fear learning processes.
The overall pooled effect sizes were:

```{r, results = "asis"}
mlm <- readRDS("mlm.RData")
tb <- lapply(1:length(processes), function(i){
  tb <- mlm[[i]]$res
  tb$Process <- processes[i]
  tb
})
tb <- do.call(rbind, tb)
tb$CI <- tidySEM::conf_int(lb = tb$ci.lb, ub = tb$ci.ub)
tb[c("ci.lb", "ci.ub")] <- NULL
names(tb) <- c("Parameter", "Estimate", "p", "Process", "CI")
kbl(tb[, c("Parameter", "Estimate", "CI", "p", "Process")], digits = 2)
```

The overall effect size estimate differed significantly from zero only for `r paste0(tb[tb$Parameter == "Overall ES", ]$Process[tb[tb$Parameter == "Overall ES", ]$p < .05], collapse = ", ")`.

The within-studies variance component $\sigma^2_w$ (between effect sizes) was significant only for `r paste0(tb[tb$Parameter == "V within", ]$Process[tb[tb$Parameter == "V within", ]$p < .05], collapse = ", ")`.

The between-studies variance $\sigma^2_b$ was significant for all processes, except `r paste0(tb[tb$Parameter == "V between", ]$Process[tb[tb$Parameter == "V between", ]$p > .05], collapse = ", ")`.

This indicates that there was substantial heterogeneity between average effect sizes across studies, but not between effect sizes published within the same studies.

## Moderator analyses

Five moderators were coded: (SSRI, frequency, disease induction, species, type of test).
The effect of these moderators was investigated using meta-regression.
Because the within-studies variance component was non-significant in most three-level analyses,
and because most studies only had one effect size, we dropped the third level in the moderator analyses.

```{r}
mods <- c("type.of.ssri", 
"frequency", "disease.induction", "species", "type.of.test")
# 
# mlm <- lapply(processes, function(p){
#   df <- dat[dat$Process == p, ]
#   modmat <- model.matrix(~., df[mods])[,-1]
#   zerocols <- colSums(modmat) == 0
#   zero <- colnames(modmat)[zerocols]
#   modmat <- modmat[, !zerocols]
#   dupcols <- which(duplicated(as.list(data.frame(modmat))))
#   dups <- colnames(modmat)[dupcols]
#   modmat <- modmat[, !dupcols]
# res <- rma(yi = df$yi,
#            vi = df$vi,
#            mods = modmat)
#vifs <- vif(res)
#})
# res <- rma(yi = dat$yi,
#            vi = dat$vi,
#            mods = model.matrix(~., data[mods])[,-1])
# vifs <- vif(res)

# write.csv(vifs, "vif.csv")
# worcs:::write_gitig(".gitignore", "!vif.csv")
```


<!-- 3. Check for multi collinearity -->

```{r results="asis", eval = FALSE}
apa_table(
aov_table, caption = "Model fit and likelihood ratio tests", note = "Significance of variance components is assessed by constraining them to zero, and examining the significance of a log-likelihood (ll) ratio test (LRT) comparing the constrained model to the full three-level model. ", placement = "p"
)

failsafe <- fsn(d, vi, data = data)
```

*A chi-square test of homogeneity of effect sizes was not significant, 2(19) 21.53, p=.37. These measures suggest considerable similarity in effect sizes across studies.*
*Consistent with this finding, moderator analysis suggested that the effect of age or sex on the overall effect size was not significant.*

```{r funnel_plot, echo=FALSE, warning=FALSE, message=FALSE, results="hide", eval = F}
png(file="funnel_plot.png",width=148, height=105, units="mm", res = 300)
funnel(model.full, main=NULL, 
       back="white", shade="white", hlines="white", xlab = "Effect size (Cohen's d)")
dev.off()
cairo_pdf("funnel_plot.pdf", family="Helvetica", width = 5.8, height = 4.1)
  funnel.rma(model.full, main=NULL, 
       back="white", shade="white", hlines="white", xlab = "Effect size (Cohen's d)", digits = 2)
dev.off()
embed_fonts("funnel_plot.pdf", outfile="funnel_plot.pdf")
beggstest <- regtest(data$d, data$vi)
sd(data$age)
```
File drawer analysis [@rosenthal_file_1979] revealed that `failsafe[["fsnum"]]` unpublished, filed, or unretrieved studies averaging null results would be required to bring the average unweighted effect size to nonsignificance. Visual inspection of the Funnel plot (Figure 3) did not clearly indicate asymmetry, which could be a sign of publication bias. Begg's test of funnel asymmetry (based on random-effects meta-analysis) similarly did not indicate significant bias, $Z = ` formatC(beggstest[["zval"]], digits = 2, format = "f")`$, $p = `formatC(beggstest[["pval"]], digits = 2, format = "f")`$.


## Moderation

We coded several potential theoretical and methodological moderators: Proportion of male participants, average age of the sample, type of population (typical or socially anxious), type of intervention, type of control condition, and outcome measure. However, the small sample size limits our ability to include these moderators in mixed-effects meta-analysis without risking overfitting (modeling random noise in the data, rather than true moderating effects). We therefore used metaforest [ @van_lissa_metaforest:_2017] to screen for relevant moderators. This technique uses the machine learning algorithm "random forests" to prevent overfitting, and to assess the importance of several potential moderators. An added benefit is that metaforest can capture non-linear relationships between moderators and effect size, and higher-order interactions. To this end, many (in this case, 10000) bootstrap samples are drawn from the original data, and a models is estimated on each bootstrap sample. Then, each model's performance is evaluated on cases not part of its bootstrap sample, yielding an estimate of explained variance in new data, $R^2_{oob}$. 
We conducted random-effects weighted metaforest, with clustered bootstrapping to account for the multilevel structure of the data  is negative, this means that the average effect size is a better predictor of out-of-bootstrap cases than the model-implied predictions. In other words, the model did not capture generalizable relationships between the moderators and effect size, and we did not find evidence for associations between the moderators and effect size.
